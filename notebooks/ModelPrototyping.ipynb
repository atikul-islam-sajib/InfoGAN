{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../data/raw/\"\n",
    "PROCESSED_PATH = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as pkl\n",
    "import os\n",
    "\n",
    "def pickle(value = None, filename = None):\n",
    "    if (value and filename) is not None:\n",
    "        pkl.dump(value = value, filename=filename)\n",
    "    else:\n",
    "        ValueError(\"Pickle is not possible due to missing arguments\".capitalize())\n",
    "        \n",
    "def clean_folder(path = None):\n",
    "    if path is not None:\n",
    "        if os.path.exists(path):\n",
    "            for file in os.listdir(path):\n",
    "                os.remove(os.path.join(path, file))\n",
    "            \n",
    "            print(\"{} - path cleaned\".format(path).capitalize())\n",
    "        else:\n",
    "            print(\"{} - path doesn't exist\".capitalize())\n",
    "    else:\n",
    "        raise ValueError(\"Clean folder is not possible due to missing arguments\".capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/ - path cleaned\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/raw/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/train-images-idx3-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "27.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/raw/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../data/raw/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "../data/processed/ - path cleaned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class Loader:\n",
    "    \"\"\"\n",
    "    A class for loading and preprocessing the MNIST dataset.\n",
    "\n",
    "    This class handles the downloading of the MNIST dataset, performs image transformations, and organizes the data into batches for training or testing.\n",
    "\n",
    "    | Parameters | Description |\n",
    "    |------------|-------------|\n",
    "    | batch_size | int, default=128. The number of samples to include in each batch of data. |\n",
    "\n",
    "    | Attributes | Description |\n",
    "    |------------|-------------|\n",
    "    | batch_size | int. The size of the batch of data. |\n",
    "\n",
    "    | Methods    | Description |\n",
    "    |------------|-------------|\n",
    "    |_do_transformation() | Applies a series of transformations to the dataset images. |\n",
    "    | download_mnist()    | Downloads the MNIST dataset, applies transformations, and organizes the data into batches. |\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> loader = Loader(batch_size=128)\n",
    "    >>> dataloader = loader.download_mnist()\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size = 128):\n",
    "        \"\"\"\n",
    "        Initializes the Loader with a specified batch size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int, optional\n",
    "            The number of samples per batch. Default is 128.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _do_transformation(self):\n",
    "        \"\"\"\n",
    "        Apply transformations to the dataset images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torchvision.transforms.Compose\n",
    "            A composed series of transformations for image processing.\n",
    "        \"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def download_mnist(self):\n",
    "        \"\"\"\n",
    "        Download the MNIST dataset and prepare it for training.\n",
    "\n",
    "        Checks for dataset existence, downloads if necessary, applies transformations, and prepares a DataLoader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            A DataLoader containing the preprocessed MNIST dataset in batches.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If any errors occur during the folder cleaning or dataset processing steps.\n",
    "        \"\"\"\n",
    "        if os.path.exists(RAW_PATH):\n",
    "            try:\n",
    "                clean_folder(path=RAW_PATH)\n",
    "            except Exception as e:\n",
    "                print(\"Exception caught in the section # {}\".format(e))\n",
    "            else:\n",
    "                dataloader = datasets.MNIST(root=os.path.join(RAW_PATH), train=True, download=True, transform=self._do_transformation())\n",
    "                dataloader = DataLoader(dataset=dataloader, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "                try:\n",
    "                    if os.path.exists(PROCESSED_PATH):\n",
    "                        try:\n",
    "                            clean_folder(path=PROCESSED_PATH)\n",
    "                        except Exception as e:\n",
    "                            print(\"Exception caught in the section # {}\".format(e))\n",
    "                        else:\n",
    "                            pickle(value = dataloader, filename = os.path.join(PROCESSED_PATH, \"dataloader.pkl\"))\n",
    "                    else:\n",
    "                        os.makedirs(PROCESSED_PATH)\n",
    "                        print(\"Processed path is created in the data folder & run the code again\".capitalize())\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Exception caught in the section # {}\".format(e))\n",
    "                else:\n",
    "                    return dataloader\n",
    "        else:\n",
    "            os.makedirs(RAW_PATH)\n",
    "            print(\"raw folder is created in the data folder and run again this code\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(batch_size=128)\n",
    "    dataloader = loader.download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A discriminator model for a Generative Adversarial Network (GAN) that classifies images as real or fake.\n",
    "\n",
    "    This model applies a series of convolutional layers to an input image and outputs a single scalar indicating the likelihood of the image being real.\n",
    "\n",
    "    | Parameters   | Description |\n",
    "    |--------------|-------------|\n",
    "    | in_channels  | int, default=1. The number of channels in the input images. |\n",
    "\n",
    "    | Attributes   | Description |\n",
    "    |--------------|-------------|\n",
    "    | in_channels  | int. The number of channels in the input images. |\n",
    "    | config_layer | list of tuples. Configuration for each layer in the model, specifying layer parameters. |\n",
    "    | model        | nn.Sequential. The sequential model comprising the discriminator's layers. |\n",
    "\n",
    "    | Methods      | Description |\n",
    "    |--------------|-------------|\n",
    "    | forward(x)   | Defines the forward pass of the discriminator. |\n",
    "    | connected_layer(config_layer) | Constructs the layers of the discriminator based on the provided configuration. |\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> discriminator = Discriminator(in_channels=1)\n",
    "    >>> image = torch.randn(64, 1, 28, 28)\n",
    "    >>> print(discriminator(image).shape)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels = 1):\n",
    "        self.in_channels = in_channels\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.config_layer = [\n",
    "            (1, 128, 4, 2, 1, 0.2, False),\n",
    "            (128, 256, 4, 2, 1, 0.2, True),\n",
    "            (256, 512, 4, 2, 1, 0.2, True),\n",
    "            (512, 1, 4, 2, 1),\n",
    "        ]\n",
    "\n",
    "        self.model = self.connected_layer(config_layer = self.config_layer)\n",
    "\n",
    "    def connected_layer(self, config_layer = None):\n",
    "        \"\"\"\n",
    "        Constructs the layers of the discriminator based on the provided configuration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        config_layer : list of tuples, optional\n",
    "            The configuration for each layer in the model. If not provided, uses the instance's config_layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nn.Sequential\n",
    "            A sequential container of the constructed layers.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If config_layer is not provided.\n",
    "        \"\"\"\n",
    "        layers = OrderedDict()\n",
    "        if config_layer is not None:\n",
    "            for index, (\n",
    "                in_channels, out_channels, kernel_size, stride, padding, slope, batch_norm) in enumerate(config_layer[:-1]):\n",
    "                layers[f\"conv_{index+1}\"] = nn.Conv2d(\n",
    "                    in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "                if batch_norm:\n",
    "                    layers[f\"batch_norm_{index+1}\"] = nn.BatchNorm2d(out_channels)\n",
    "                layers[f\"leaky_relu_{index+1}\"] = nn.LeakyReLU(slope)\n",
    "\n",
    "            (in_channels, out_channels, kernel_size, stride, padding) = config_layer[-1]\n",
    "            layers[\"out_conv\"] = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            layers[\"out_sigmoid\"] = nn.Sigmoid()\n",
    "\n",
    "            return nn.Sequential(layers)\n",
    "        else:\n",
    "            raise Exception(\"Config layer is not passed\".capitalize())\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            x = self.model(x)\n",
    "            return x.view(x.size(0), -1)\n",
    "        else:\n",
    "            raise Exception(\"Input is not passed\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    discriminator = Discriminator(in_channels = 1)\n",
    "    \n",
    "    image = torch.randn(64, 1, 28, 28)\n",
    "    print(discriminator(image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space = 100):\n",
    "        self.latent_space = latent_space\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.layers_config = [\n",
    "            (self.latent_space, 512, 4, 1, 0, True),\n",
    "            (512, 256, 4, 2, 1, True),\n",
    "            (256, 128, 4, 2, 1, True),\n",
    "            (128, 1, 4, 2, 1)\n",
    "        ]\n",
    "        \n",
    "        self.model = self.connected_layer(layers_config = self.layers_config)\n",
    "    def connected_layer(self, layers_config = None):\n",
    "        layers = OrderedDict()\n",
    "        if layers_config is not None:\n",
    "            for index, (in_channels, out_channels, kernel_size, stride, padding, batch_norm) in enumerate(layers_config[:-1]):\n",
    "                layers[\"conv_transpose_{}\".format(index+1)] = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n",
    "                if batch_norm:\n",
    "                    layers[\"batch_norm_{}\".format(index+1)] = nn.BatchNorm2d(num_features = out_channels)\n",
    "                \n",
    "                layers[f\"relu_{index+1}\"] = nn.ReLU(inplace = True)\n",
    "                \n",
    "            (in_channels, out_channels, kernel_size, stride, padding) = layers_config[-1]\n",
    "            layers[f\"out_conv\"] = nn.ConvTranspose2d(\n",
    "                in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n",
    "            layers[f\"out_tanh\"] = nn.Tanh()\n",
    "            \n",
    "            return nn.Sequential(layers)\n",
    "                \n",
    "        else:\n",
    "            raise Exception(\"Config layer is not passed\".capitalize())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x is not None:\n",
    "            x = self.model(x)\n",
    "            return x\n",
    "        else:\n",
    "            raise Exception(\"Input is not passed\".capitalize())\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    generator = Generator(latent_space = 100)\n",
    "    noise_data = torch.randn(64, 100, 1, 1)\n",
    "    print(generator(noise_data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class QNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model designed for Q-learning tasks, featuring convolutional layers for feature extraction and fully connected layers for action value prediction.\n",
    "\n",
    "    The model processes input images through convolutional layers followed by fully connected layers to predict action values for each possible action, suitable for reinforcement learning environments.\n",
    "\n",
    "    | Parameters   | Description |\n",
    "    |--------------|-------------|\n",
    "    | (None)       | This class does not take parameters at initialization. |\n",
    "\n",
    "    | Attributes   | Description |\n",
    "    |--------------|-------------|\n",
    "    | layers_config| list of tuples. Configuration for convolutional layers specifying in_channels, out_channels, kernel_size, stride, padding, and whether to use batch normalization. |\n",
    "    | model        | nn.Sequential. The sequential model comprising the convolutional layers. |\n",
    "    | fc_layer     | nn.Sequential. The sequential model comprising the fully connected layers for action value prediction. |\n",
    "\n",
    "    | Methods      | Description |\n",
    "    |--------------|-------------|\n",
    "    | forward(x)   | Defines the forward pass through the convolutional and fully connected layers. |\n",
    "    | connected_layer(layers_config) | Constructs the convolutional layers based on the provided configuration. |\n",
    "    | connected_fc_layer(in_features) | Constructs the fully connected layers for action value prediction. |\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> qnet = QNet()\n",
    "    >>> image = torch.randn(64, 1, 32, 32)\n",
    "    >>> print(qnet(image).shape)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers_config = [\n",
    "            (1, 128, 4, 2, 1, True),\n",
    "            (128, 64, 4, 2, 1, True),\n",
    "        ]\n",
    "        \n",
    "        self.model = self.connected_layer(layers_config = self.layers_config)\n",
    "        self.fc_layer = self.connected_fc_layer(in_features = 8*8*64)\n",
    "    \n",
    "    def connected_layer(self, layers_config = None):\n",
    "        \"\"\"\n",
    "        Constructs the convolutional layers of the model based on the provided configuration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layers_config : list of tuples, optional\n",
    "            Configuration for each convolutional layer. If not provided, uses the instance's layers_config.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nn.Sequential\n",
    "            A sequential container of the constructed convolutional layers.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If layers_config is not provided.\n",
    "        \"\"\"\n",
    "        layers = OrderedDict()\n",
    "        if layers_config is not None:\n",
    "            for index, (in_channels, out_channels, kernel_size, stride, padding, use_norm) in enumerate(layers_config):\n",
    "                layers[f\"conv_{index+1}\"] = nn.Conv2d(\n",
    "                    in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride, padding = padding)\n",
    "                if use_norm:\n",
    "                    layers[f\"batchnorm_{index+1}\"] = nn.BatchNorm2d(out_channels)\n",
    "                \n",
    "                layers[f\"relu_{index+1}\"] = nn.ReLU(inplace = True)\n",
    "                \n",
    "            return nn.Sequential(layers)\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Config layer is not passed\".capitalize())\n",
    "    \n",
    "    def connected_fc_layer(self, in_features = None):\n",
    "        \"\"\"\n",
    "        Constructs the fully connected layers of the model for action value prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int, optional\n",
    "            The number of input features to the first fully connected layer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nn.Sequential\n",
    "            A sequential container of the constructed fully connected layers.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If in_features is not provided.\n",
    "        \"\"\"\n",
    "        layers = OrderedDict()\n",
    "        layers[f\"fc_1\"] = nn.Linear(in_features = in_features, out_features = 10)\n",
    "        layers[f\"fc_soft\"] = nn.Softmax(dim = 1)\n",
    "        \n",
    "        return nn.Sequential(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor for the model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor after processing through the convolutional and fully connected layers.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If the input tensor x is not provided.\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            x = self.model(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc_layer(x)\n",
    "            return x\n",
    "        else:\n",
    "            raise Exception(\"Input is not passed\".capitalize())\n",
    "        \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    qnet = QNet()\n",
    "    \n",
    "    image = torch.randn(64, 1, 32, 32)\n",
    "    print(qnet(image).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
