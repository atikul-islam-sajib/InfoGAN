{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../data/raw/\"\n",
    "PROCESSED_PATH = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib as pkl\n",
    "import os\n",
    "\n",
    "def pickle(value = None, filename = None):\n",
    "    if (value and filename) is not None:\n",
    "        pkl.dump(value = value, filename=filename)\n",
    "    else:\n",
    "        ValueError(\"Pickle is not possible due to missing arguments\".capitalize())\n",
    "        \n",
    "def clean_folder(path = None):\n",
    "    if path is not None:\n",
    "        if os.path.exists(path):\n",
    "            for file in os.listdir(path):\n",
    "                os.remove(os.path.join(path, file))\n",
    "            \n",
    "            print(\"{} - path cleaned\".format(path).capitalize())\n",
    "        else:\n",
    "            print(\"{} - path doesn't exist\".capitalize())\n",
    "    else:\n",
    "        raise ValueError(\"Clean folder is not possible due to missing arguments\".capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/ - path cleaned\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/raw/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/train-images-idx3-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "27.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/raw/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/raw/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ../data/raw/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/raw/MNIST/raw\n",
      "\n",
      "../data/processed/ - path cleaned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class Loader:\n",
    "    \"\"\"\n",
    "    A class for loading and preprocessing the MNIST dataset.\n",
    "\n",
    "    This class handles the downloading of the MNIST dataset, performs image transformations, and organizes the data into batches for training or testing.\n",
    "\n",
    "    | Parameters | Description |\n",
    "    |------------|-------------|\n",
    "    | batch_size | int, default=128. The number of samples to include in each batch of data. |\n",
    "\n",
    "    | Attributes | Description |\n",
    "    |------------|-------------|\n",
    "    | batch_size | int. The size of the batch of data. |\n",
    "\n",
    "    | Methods    | Description |\n",
    "    |------------|-------------|\n",
    "    |_do_transformation() | Applies a series of transformations to the dataset images. |\n",
    "    | download_mnist()    | Downloads the MNIST dataset, applies transformations, and organizes the data into batches. |\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> loader = Loader(batch_size=128)\n",
    "    >>> dataloader = loader.download_mnist()\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size = 128):\n",
    "        \"\"\"\n",
    "        Initializes the Loader with a specified batch size.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int, optional\n",
    "            The number of samples per batch. Default is 128.\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _do_transformation(self):\n",
    "        \"\"\"\n",
    "        Apply transformations to the dataset images.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torchvision.transforms.Compose\n",
    "            A composed series of transformations for image processing.\n",
    "        \"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def download_mnist(self):\n",
    "        \"\"\"\n",
    "        Download the MNIST dataset and prepare it for training.\n",
    "\n",
    "        Checks for dataset existence, downloads if necessary, applies transformations, and prepares a DataLoader.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.utils.data.DataLoader\n",
    "            A DataLoader containing the preprocessed MNIST dataset in batches.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        Exception\n",
    "            If any errors occur during the folder cleaning or dataset processing steps.\n",
    "        \"\"\"\n",
    "        if os.path.exists(RAW_PATH):\n",
    "            try:\n",
    "                clean_folder(path=RAW_PATH)\n",
    "            except Exception as e:\n",
    "                print(\"Exception caught in the section # {}\".format(e))\n",
    "            else:\n",
    "                dataloader = datasets.MNIST(root=os.path.join(RAW_PATH), train=True, download=True, transform=self._do_transformation())\n",
    "                dataloader = DataLoader(dataset=dataloader, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "                try:\n",
    "                    if os.path.exists(PROCESSED_PATH):\n",
    "                        try:\n",
    "                            clean_folder(path=PROCESSED_PATH)\n",
    "                        except Exception as e:\n",
    "                            print(\"Exception caught in the section # {}\".format(e))\n",
    "                        else:\n",
    "                            pickle(value = dataloader, filename = os.path.join(PROCESSED_PATH, \"dataloader.pkl\"))\n",
    "                    else:\n",
    "                        os.makedirs(PROCESSED_PATH)\n",
    "                        print(\"Processed path is created in the data folder & run the code again\".capitalize())\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Exception caught in the section # {}\".format(e))\n",
    "                else:\n",
    "                    return dataloader\n",
    "        else:\n",
    "            os.makedirs(RAW_PATH)\n",
    "            print(\"raw folder is created in the data folder and run again this code\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loader = Loader(batch_size=128)\n",
    "    dataloader = loader.download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
